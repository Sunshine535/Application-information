{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oFf7zmaJs4_5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 网易云api爬虫，api挂掉了，之后更新"
      ],
      "metadata": {
        "id": "oFf7zmaJs4_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.core.display import clear_output\n",
        "#!pip install aiohttp\n",
        "#!pip install nest_asyncio\n",
        "#clear_output()\n"
      ],
      "metadata": {
        "id": "RqQuhBkofLKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import requests\n",
        "#import json\n",
        "#import aiohttp\n",
        "#import asyncio\n",
        "#import nest_asyncio\n",
        "#from tqdm import tqdm\n",
        "\n",
        "\"\"\"nest_asyncio.apply()\n",
        "\n",
        "async def fetch(client, trackIds):\n",
        "  async with client.get(f'http://music.eleuu.com/lyric?id={trackIds}') as resp:\n",
        "    assert resp.status == 200\n",
        "    return await resp.text()\n",
        "\n",
        "async def main(tracks, results):\n",
        "  async with aiohttp.ClientSession() as client:\n",
        "    for item in tqdm(tracks):\n",
        "      response = await fetch(client, trackIds=item['id'])\n",
        "      data = json.loads(response)\n",
        "      if 'lyric' in data:\n",
        "        results.append(data['lyric'])\n",
        "\n",
        "playlist_id = 7216756628\n",
        "url_path = f'http://music.eleuu.com/playlist/detail?id={playlist_id}'\n",
        "\n",
        "r = requests.get(url_path)\n",
        "data = json.loads(r.text)\n",
        "\n",
        "lyric_results = []\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(main(data['result']['tracks'], results=lyric_results))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BVAU1A5dHt6",
        "outputId": "b5edbb86-03dc-46eb-94cb-56ad5c66c595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-6ee4f0b3afe8>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    loop.run_until_complete(main(data['result']['tracks'], results=lyric_results))\u001b[0m\n\u001b[0m                                                                                  \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import re\n",
        "\n",
        "def clean_lyric(lyrics):\n",
        "  example = [\n",
        "             re.sub('[\\[\\(（《]》）', '', s) for s in lyrics.splitlines()\n",
        "    if s.strip()\n",
        "  ]\n",
        "  example = [\n",
        "             x.strip() for x in example\n",
        "             if ':' not in x and ':' not in x and x.strip()\n",
        "             and re.findall(r'', x)\n",
        "  ]\n",
        "\n",
        "  if len(example) < 4:\n",
        "    return None\n",
        "  lyric_string = '\\n'.join(example)\n",
        "  return lyric_string\n",
        "\n",
        "cleaned_lyrics = [clean_lyric(lyric) for lyric in lyric_results]\n",
        "\n",
        "filtered_lyrics = list(filter(lambda x: x, cleaned_lyrics))"
      ],
      "metadata": {
        "id": "u5_eEnQ7RcLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58kD9vbAARZ"
      },
      "source": [
        "# 1. 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7QADQvokUzH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c47bd574-72e8-45f1-d685-eb2ee85dc1b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              " function ClickConnect(){\n",
              "   btn = document.querySelector(\"colab-connect-button\")\n",
              "   if (btn != null){\n",
              "     console.log(\"Click colab-connect-button\"); \n",
              "     btn.click() \n",
              "     }\n",
              "   \n",
              "   btn = document.getElementById('ok')\n",
              "   if (btn != null){\n",
              "     console.log(\"Click reconnect\"); \n",
              "     btn.click() \n",
              "     }\n",
              "  }\n",
              "  \n",
              "setInterval(ClickConnect,60000)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# 用来保持连接的\n",
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\"); \n",
        "     btn.click() \n",
        "     }\n",
        "   \n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\"); \n",
        "     btn.click() \n",
        "     }\n",
        "  }\n",
        "  \n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inPo4CMrAGMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43682f2e-c054-4309-c397-849dfbb46dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPT2-Chinese'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Total 280 (delta 0), reused 0 (delta 0), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (280/280), 13.44 MiB | 26.46 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "# 克隆的仓库在./GPT2-Chinese 目录下\n",
        "!git clone https://github.com/Morizeyao/GPT2-Chinese.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx3q7ez0VGY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "435d361d-85d3-41b0-ac76-1da8102c4f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.1.1\n",
            "  Downloading transformers-2.1.1-py3-none-any.whl (311 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 311 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 2)) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 5)) (0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 6)) (2.7.0)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.8.0a20220104-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 8)) (0.16.0)\n",
            "Collecting thulac\n",
            "  Downloading thulac-0.2.1.tar.gz (52.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 52.9 MB 74 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.28-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r ./GPT2-Chinese/requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.42.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.1.1)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.28\n",
            "  Downloading botocore-1.23.28-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.28->boto3->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (1.4.1)\n",
            "Building wheels for collected packages: thulac\n",
            "  Building wheel for thulac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thulac: filename=thulac-0.2.1-py3-none-any.whl size=53141672 sha256=7dec8f0fa47114349a6134efdfc58954224f2ce7d58af3147877fd9ecc775de7\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/37/f3/be4ae10faf0fbf35cc192469b737ead6f8f99404bcd82fb2e0\n",
            "Successfully built thulac\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, transformers, thulac, tb-nightly\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.28 botocore-1.23.28 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 tb-nightly-2.8.0a20220104 thulac-0.2.1 transformers-2.1.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "# 安装需求文件\n",
        "!python3 -m pip install -r ./GPT2-Chinese/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_uVk6TyGXGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b359c1c3-4a0e-4fdb-9f3e-332e3ed328ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese\n"
          ]
        }
      ],
      "source": [
        "# 移动到GPT2-Chinese 路径\n",
        "%cd ./GPT2-Chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEYIRUGxCZpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa91d1b-a7a1-4fe8-90bd-5272f81ff6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese/pretrained\n"
          ]
        }
      ],
      "source": [
        "!mkdir pretrained\n",
        "%cd ./pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2bJSuu_HvXg"
      },
      "source": [
        "**下载pretrained model 到新创建的pretrained文件夹**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl0gWAfpApOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499eaea1-24aa-421a-b1f8-716a2312c1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-D41f3ubMmRhhTKm-y6UPWsxn79jTleL\n",
            "To: /content/GPT2-Chinese/pretrained/config.json\n",
            "100% 912/912 [00:00<00:00, 1.77MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-FaHeyfqOKBgD2fra_Q9XF6rxIbi-VJc\n",
            "To: /content/GPT2-Chinese/pretrained/pytorch_model.bin\n",
            "100% 459M/459M [00:02<00:00, 163MB/s]\n",
            "config.json  pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "#!gdown --id 1yzoTtYy62zuTHUvGQOuWhLQJu51TWhOX\n",
        "#!gdown --id 1MQXvJr3DXCvdCL1yNcvXU9FWn4Yt3ny2\n",
        "!gdown --id 1-D41f3ubMmRhhTKm-y6UPWsxn79jTleL\n",
        "!gdown --id 1-FaHeyfqOKBgD2fra_Q9XF6rxIbi-VJc\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkBLFhPjHAgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fef7885-38bd-4587-dc5e-0d85f2b71425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlujTb5ZHKty"
      },
      "source": [
        "# 2. 转换训练用文本\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f train.json\n",
        "!mkdir data"
      ],
      "metadata": {
        "id": "OfV2QH5Vb0Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kVtKWjaIh_5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dic = {}\n",
        "with open(\"train.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    merge_line = \"\"\n",
        "    for line in f:\n",
        "        merge_line += line\n",
        "        if len(merge_line) > 300:\n",
        "            dic[merge_line] = 1\n",
        "            merge_line = \"\"\n",
        "\n",
        "with open(\"./data/train.json\", \"w\", encoding=\"utf8\") as f:\n",
        "    json.dump(dic, f, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NXbDOrrJBjY"
      },
      "source": [
        "# 3. 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb_LDuK4JPGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb15bcf-cb14-4774-bf6f-d10dbf00f646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-05 07:37:59.256587: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "usage: train.py [-h] [--device DEVICE] [--model_config MODEL_CONFIG]\n",
            "                [--tokenizer_path TOKENIZER_PATH]\n",
            "                [--raw_data_path RAW_DATA_PATH]\n",
            "                [--tokenized_data_path TOKENIZED_DATA_PATH] [--raw]\n",
            "                [--epochs EPOCHS] [--batch_size BATCH_SIZE] [--lr LR]\n",
            "                [--warmup_steps WARMUP_STEPS] [--log_step LOG_STEP]\n",
            "                [--stride STRIDE]\n",
            "                [--gradient_accumulation GRADIENT_ACCUMULATION] [--fp16]\n",
            "                [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                [--max_grad_norm MAX_GRAD_NORM] [--num_pieces NUM_PIECES]\n",
            "                [--min_length MIN_LENGTH] [--output_dir OUTPUT_DIR]\n",
            "                [--pretrained_model PRETRAINED_MODEL]\n",
            "                [--writer_dir WRITER_DIR] [--segment] [--bpe_token]\n",
            "                [--encoder_json ENCODER_JSON] [--vocab_bpe VOCAB_BPE]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --device DEVICE       设置使用哪些显卡\n",
            "  --model_config MODEL_CONFIG\n",
            "                        选择模型参数\n",
            "  --tokenizer_path TOKENIZER_PATH\n",
            "                        选择词库\n",
            "  --raw_data_path RAW_DATA_PATH\n",
            "                        原始训练语料\n",
            "  --tokenized_data_path TOKENIZED_DATA_PATH\n",
            "                        tokenized语料存放位置\n",
            "  --raw                 是否先做tokenize\n",
            "  --epochs EPOCHS       训练循环\n",
            "  --batch_size BATCH_SIZE\n",
            "                        训练batch size\n",
            "  --lr LR               学习率\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        warm up步数\n",
            "  --log_step LOG_STEP   多少步汇报一次loss，设置为gradient accumulation的整数倍\n",
            "  --stride STRIDE       训练时取训练数据的窗口步长\n",
            "  --gradient_accumulation GRADIENT_ACCUMULATION\n",
            "                        梯度积累\n",
            "  --fp16                混合精度\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "  --num_pieces NUM_PIECES\n",
            "                        将训练语料分成多少份\n",
            "  --min_length MIN_LENGTH\n",
            "                        最短收录文章长度\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        模型输出路径\n",
            "  --pretrained_model PRETRAINED_MODEL\n",
            "                        模型训练起点路径\n",
            "  --writer_dir WRITER_DIR\n",
            "                        Tensorboard路径\n",
            "  --segment             中文以词为单位\n",
            "  --bpe_token           subword\n",
            "  --encoder_json ENCODER_JSON\n",
            "                        encoder.json\n",
            "  --vocab_bpe VOCAB_BPE\n",
            "                        vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# 参数用途\n",
        "!python train.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9z9-HKiUwmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54ef7fe-ce7e-4333-865f-e62a1440beb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-05 07:38:09.548636: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=1, bpe_token=False, device='0,1,2,3', encoder_json='tokenizations/encoder.json', epochs=1, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, log_step=1, lr=0.00015, max_grad_norm=1.0, min_length=3, model_config='config/model_config_small.json', num_pieces=33, output_dir='model/', pretrained_model='pretrained', raw=True, raw_data_path='data/train.json', segment=False, stride=768, tokenized_data_path='data/tokenized/', tokenizer_path='cache/vocab_small.txt', vocab_bpe='tokenizations/vocab.bpe', warmup_steps=2000, writer_dir='tensorboard_summary/')\n",
            "config:\n",
            "{\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 10,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 13317\n",
            "}\n",
            "\n",
            "using device: cuda\n",
            "building files\n",
            "reading lines\n",
            "100% 33/33 [00:00<00:00, 456.36it/s]\n",
            "finish\n",
            "files built\n",
            "number of parameters: 102068736\n",
            "calculating total steps\n",
            "100% 33/33 [00:00<00:00, 7656.80it/s]\n",
            "total steps = 12\n",
            "starting training\n",
            "epoch 1\n",
            "time: 2022-01-05 07:38:22.933923\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "now time: 7:38. Step 1 of piece 24 of epoch 1, loss 0.069224052131176\n",
            "now time: 7:38. Step 2 of piece 24 of epoch 1, loss 0.09506738185882568\n",
            "now time: 7:38. Step 3 of piece 24 of epoch 1, loss 0.18275192379951477\n",
            "now time: 7:38. Step 4 of piece 24 of epoch 1, loss 0.07436362653970718\n",
            "now time: 7:38. Step 5 of piece 24 of epoch 1, loss 0.056224074214696884\n",
            "now time: 7:38. Step 6 of piece 24 of epoch 1, loss 0.075898177921772\n",
            "now time: 7:38. Step 7 of piece 24 of epoch 1, loss 0.12011440843343735\n",
            "now time: 7:38. Step 8 of piece 24 of epoch 1, loss 0.05132909491658211\n",
            "now time: 7:38. Step 9 of piece 24 of epoch 1, loss 0.08508315682411194\n",
            "now time: 7:38. Step 10 of piece 24 of epoch 1, loss 0.05132044479250908\n",
            "now time: 7:38. Step 11 of piece 24 of epoch 1, loss 0.10897300392389297\n",
            "now time: 7:38. Step 12 of piece 24 of epoch 1, loss 0.04221245273947716\n",
            "now time: 7:38. Step 13 of piece 24 of epoch 1, loss 0.07956259697675705\n",
            "saving model for epoch 1\n",
            "epoch 1 finished\n",
            "time: 2022-01-05 07:38:33.018374\n",
            "time for one epoch: 0:00:10.084451\n",
            "training finished\n"
          ]
        }
      ],
      "source": [
        "# 超过memory就把batch_size改小点\n",
        "!python train.py --raw --epochs 1 --batch_size 1 --num_pieces 33 --min_length 3 --pretrained_model pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfsAF8j1J4xw"
      },
      "source": [
        "# 4. 生成文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kekMcNGMc9n0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad61804-3da8-4551-d46f-ad36955a4586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-05 07:38:50.570679: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=1, device='0,1,2,3', fast_pattern=True, length=400, model_config='config/model_config_small.json', model_path='model/final_model', no_wordpiece=False, nsamples=3, prefix='我是谁', repetition_penalty=1.0, save_samples=False, save_samples_path='.', segment=False, temperature=1, tokenizer_path='cache/vocab_small.txt', topk=8, topp=0)\n",
            "100% 400/400 [00:07<00:00, 55.91it/s]\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "我是谁，无所谓，不管好与坏，紧握着手中烟的手，再多的苦也不闪躲，就像你离去那天，满天的烟火\n",
            "\n",
            "\n",
            "〓[UNK]无所谓，不管好与坏，紧握着手中烟的手，再多的苦也不闪躲\n",
            "just，like，a，star\n",
            "再多的困难，我们也不会怕犯错\n",
            "坚强的外壳，是你扑向我的心里留下的痕迹，just，like，a，star\n",
            "就像你离去那天，满天的烟火\n",
            "再多的困难我们也不会怕犯错\n",
            "坚强的外壳，是你扑向我的心里留下的痕迹\n",
            "try，to，smiling，坚强的外壳，是你扑向我的心里留下的痕迹\n",
            "try，to，smiling\n",
            "坚强的外壳\n",
            "是你扑向我的心里留下的痕迹\n",
            "try，to，smiling\n",
            "坚强的外壳\n",
            "是你扑向我的心里留下的痕迹\n",
            "try，to，smiling\n",
            "最后的一个拥抱，是你离去时留下的微笑\n",
            "最后的一个拥抱\n",
            "是你离去时留下的微笑\n",
            "最后的一个拥抱\n",
            "是你离去时留下的微笑\n",
            "\n",
            "\n",
            "〓[UNK]\n",
            "\n",
            "\n",
            "〓[UNK]\n",
            "\n",
            "\n",
            "〓[UNK]\n",
            "最后的一个拥抱\n",
            "是你离去时留下的微笑\n",
            "留下的微笑\n",
            "留下的微笑\n",
            "留下的微笑\n",
            "留下的微笑\n",
            "\n",
            "\n",
            "∩\n",
            "∩\n",
            "∩\n",
            "∩\n",
            "°\n",
            "\n",
            "\n",
            "\n",
            "→§§§\n",
            "100% 400/400 [00:07<00:00, 55.76it/s]\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "我是谁的替身人前人后我是谁的替身人后我是谁的替身人前人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身\n",
            "人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "\n",
            "\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是谁的替身人后我是谁的替身\n",
            "人前人后我是谁的替身人后我是谁的替身人后我是\n",
            "100% 400/400 [00:07<00:00, 55.28it/s]\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "我是谁，为何不说我爱你眼神中飘着不安的情怀oh～，总是在分分秒秒难以入睡的时候我在想你我是谁，是不是你在敷衍我做梦也会有些迷惘oh～\n",
            "你忘了我也没有错我们都有错把承诺当有趣爱人最后都变成陌生人oh～\n",
            "人生总有挫折总难免会有起伏oh～～～放弃是为了更好的明天oh～～\n",
            "拥抱过哭过笑过也值得纪念的回忆都还记得oh～～～～～my，love，is，like，a，princess\n",
            "爱人别忘了我们都有过错把承诺当有趣爱人最后都变成陌生人\n",
            "人生总有挫折总难免会有起伏oh～～～～～放弃是为了更好的明天oh～～～my，love，is，like，a，princess\n",
            "拥抱过哭过笑过也值得纪念的回忆都还记得oh～～～～my，love，is，like，a，princess\n",
            "爱人别忘了我们都有过错把承诺当有趣爱人最后都变成陌生人\n",
            "\n",
            "\n",
            "happy，birthday\n",
            "you，have，a，princess\n",
            "try，to，have，a，chance\n",
            "have，a，princess\n",
            "have，a，chance\n",
            "为了更好的明天好好对待\n",
            "我为自己留下了空白让悲伤留白不代表我还爱你\n",
            "虽然心中早就有了依赖却还是有些小小\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# length决定长度， nsamples 后面决定生成多少个例子。\n",
        "input = \"我是谁\"\n",
        "!python3 generate.py --length=400 --nsamples=3 --prefix=$input --fast_pattern"
      ]
    }
  ]
}